{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge: Using Starbucks app user data to predict effective offers\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In my capstone project, I aim to answer 2 main business questions:\n",
    "1. What are the main drivers of an effective offer on the Starbucks app? \n",
    "2. Could the data provided, namely offer characteristics and user demographics, predict whether a user would take up an offer? \n",
    "\n",
    "The data used in this capstone project was provided by Udacity as part of the Data Scientist Nanodegree course. It consists of simulated data that emulates customer behavior on the Starbucks rewards mobile app.\n",
    "\n",
    "Background information on the mobile app indicates that Starbucks sends out offers to users of the app once every few days. However, some users may not receive any offers during certain weeks, and not all users receive the same offer.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The objective of this project is to address two main business questions related to the effectiveness of offers on the Starbucks app. The first question aims to identify the key drivers of offer effectiveness, while the second question explores whether it is possible to predict whether a user would take up an offer using data on user demographics and offer characteristics.\n",
    "\n",
    "The data provided consists of 3 datasets:\n",
    "- Offer portfolio, which consists of the attributes of each offer\n",
    "- Demographic data for each customer\n",
    "- Transactional records of events occurring on the app\n",
    "\n",
    "To address the two main business questions, the data provided was used to develop three classification supervised machine learning models, each utilizing data from one of the three different offer types. \n",
    "\n",
    "The models were used to identify the key drivers of offer effectiveness and to explore whether they could predict offer uptake. Additionally, the project also examined the characteristics of users who did or did not take up an offer. \n",
    "\n",
    "Two additional models were also created to explore whether an all-in-one model could be used as a replacement for the three separate models, with the offer types functioning as a categorical variable, and to predict the amount a user would spend given that the offer effectively influenced them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "%matplotlib inline\n",
    "\n",
    "# read in the json files\n",
    "portfolio = pd.read_json('portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Data\n",
    "\n",
    "### a. give portfolio information\n",
    "\n",
    "The schema is as follows, per the information provided by Udacity:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - \n",
    "* channels (list of strings)\n",
    "\n",
    "Moreover, some further information given about the offers is that there are 3 different offer types:\n",
    "- BOGO - buy one get one free\n",
    "- Discount - discount with purchase\n",
    "- Informational - provides information about products\n",
    "\n",
    "As a result, the schema is rather simple since it provides the characteristics of three separate offer kinds. Although the period was not specified, I inferred from the context that it was expressed in terms of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reward</th>\n",
       "      <th>channels</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>[email, mobile, social]</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>bogo</td>\n",
       "      <td>ae264e3637204a6fb9bb56bc8210ddfd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>[web, email, mobile, social]</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>bogo</td>\n",
       "      <td>4d5c57ea9a6940dd891ad53e9dbe8da0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>[web, email, mobile]</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>bogo</td>\n",
       "      <td>9b98b8c7a33c4b65b9aebfe6a799e6d9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[web, email]</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>discount</td>\n",
       "      <td>0b1e1539f2cc45b7b9fa7c272da2e1d7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reward                      channels  difficulty  duration     offer_type  \\\n",
       "0      10       [email, mobile, social]          10         7           bogo   \n",
       "1      10  [web, email, mobile, social]          10         5           bogo   \n",
       "2       0          [web, email, mobile]           0         4  informational   \n",
       "3       5          [web, email, mobile]           5         7           bogo   \n",
       "4       5                  [web, email]          20        10       discount   \n",
       "\n",
       "                                 id  \n",
       "0  ae264e3637204a6fb9bb56bc8210ddfd  \n",
       "1  4d5c57ea9a6940dd891ad53e9dbe8da0  \n",
       "2  3f207df678b143eea3cee63160fa8bed  \n",
       "3  9b98b8c7a33c4b65b9aebfe6a799e6d9  \n",
       "4  0b1e1539f2cc45b7b9fa7c272da2e1d7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists that are nested are found in the channels column. I therefore notice that in order for the column to become categorical variables in my dataset, I will need to expand it later on during preprocessing. I also observe that each scale is different; for instance, the difficulty is measured in dollars, whereas the time is measured in days. Therefore, some feature scaling will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reward        0\n",
       "channels      0\n",
       "difficulty    0\n",
       "duration      0\n",
       "offer_type    0\n",
       "id            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values\n",
    "portfolio.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that this dataset has no missing values, so we won't need to impute any values or decide how to get rid of them during preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of unique offers\n",
    "portfolio.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "offer_type\n",
       "bogo             4\n",
       "discount         4\n",
       "informational    2\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio.groupby('offer_type')['id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 different offer ids that we need to remember, including 2 informative types and 4 bogo and discount types apiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data on demographics\n",
    "\n",
    "The 'profile' dataset contains consumer demographic information. The variables and schema are as follows: \n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "Given that it includes the customer's demographic profile, it is also rather simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>118</td>\n",
       "      <td>68be06ca386d4c31939f3a4f0e3dd783</td>\n",
       "      <td>20170212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>55</td>\n",
       "      <td>0610b486422d4921ae7d2bf64640c50b</td>\n",
       "      <td>20170715</td>\n",
       "      <td>112000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>118</td>\n",
       "      <td>38fe809add3b4fcf9315a9694bb96ff5</td>\n",
       "      <td>20180712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>75</td>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>20170509</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>118</td>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>20170804</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  age                                id  became_member_on    income\n",
       "0   None  118  68be06ca386d4c31939f3a4f0e3dd783          20170212       NaN\n",
       "1      F   55  0610b486422d4921ae7d2bf64640c50b          20170715  112000.0\n",
       "2   None  118  38fe809add3b4fcf9315a9694bb96ff5          20180712       NaN\n",
       "3      F   75  78afa995795e4d85b5d9ceeca43f5fef          20170509  100000.0\n",
       "4   None  118  a03223e636434f42ac4c3df47e8bac43          20170804       NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gender and income columns in the first five lines already have some null values, and the age column has some values that don't make sense (like 118)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender              2175\n",
       "age                    0\n",
       "id                     0\n",
       "became_member_on       0\n",
       "income              2175\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "profile.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, the `gender` and `income` have null values. Some good news is that whichever values are null in `gender` are also null in `income`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check distribution of age column\n",
    "bins = pd.cut(profile.age, bins=range(0, 120, 10))\n",
    "bins.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the `age = 118` value does not make sense there as it is clearly out of the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of age=118:  gender                 0\n",
      "age                 2175\n",
      "id                  2175\n",
      "became_member_on    2175\n",
      "income                 0\n",
      "dtype: int64\n",
      "  gender  income  age\n",
      "0   None     NaN  118\n",
      "2   None     NaN  118\n",
      "4   None     NaN  118\n",
      "6   None     NaN  118\n",
      "7   None     NaN  118\n"
     ]
    }
   ],
   "source": [
    "# Check count of age=118 value\n",
    "print(\"Count of age=118: \", profile[profile['age']==118].count())\n",
    "\n",
    "# Check corresponding gender and income columns\n",
    "print(profile[['gender','income','age']][profile['age']==118].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information above, we can observe that the age=118 values in the profile dataframe correspond to missing values in the gender and income columns. As a result, during data preprocessing, we can consider dropping these rows if they do not represent a significant proportion of the data. This would allow us to avoid potential issues related to missing data and ensure that the remaining data is as accurate and reliable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check how many unique people are in dataset\n",
    "profile['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check distributions of income\n",
    "bins = pd.cut(profile.income, bins=range(0, 140000, 10000))\n",
    "bins.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The became_member_on column in the profile dataframe could potentially be feature engineered to calculate the tenure of membership in days, which could be a useful feature in determining the effectiveness of offers on the Starbucks app. By calculating the tenure of membership, we could potentially identify whether users who have been members for a longer period of time are more likely to respond positively to offers. Therefore, feature engineering the became_member_on column to calculate the tenure of membership in days could provide valuable insights into user behavior on the Starbucks app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20170212\n",
       "1    20170715\n",
       "2    20180712\n",
       "3    20170509\n",
       "4    20170804\n",
       "Name: became_member_on, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile.became_member_on.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Transactional records\n",
    "\n",
    "The schema for the transactional data is as follows:\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>event</th>\n",
       "      <th>value</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78afa995795e4d85b5d9ceeca43f5fef</td>\n",
       "      <td>offer received</td>\n",
       "      <td>{'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a03223e636434f42ac4c3df47e8bac43</td>\n",
       "      <td>offer received</td>\n",
       "      <td>{'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e2127556f4f64592b11af22de27a7932</td>\n",
       "      <td>offer received</td>\n",
       "      <td>{'offer id': '2906b810c7d4411798c6938adc9daaa5'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8ec6ce2a7e7949b1bf142def7d0e0586</td>\n",
       "      <td>offer received</td>\n",
       "      <td>{'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68617ca6246f4fbc85e91a2a49552598</td>\n",
       "      <td>offer received</td>\n",
       "      <td>{'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person           event  \\\n",
       "0  78afa995795e4d85b5d9ceeca43f5fef  offer received   \n",
       "1  a03223e636434f42ac4c3df47e8bac43  offer received   \n",
       "2  e2127556f4f64592b11af22de27a7932  offer received   \n",
       "3  8ec6ce2a7e7949b1bf142def7d0e0586  offer received   \n",
       "4  68617ca6246f4fbc85e91a2a49552598  offer received   \n",
       "\n",
       "                                              value  time  \n",
       "0  {'offer id': '9b98b8c7a33c4b65b9aebfe6a799e6d9'}     0  \n",
       "1  {'offer id': '0b1e1539f2cc45b7b9fa7c272da2e1d7'}     0  \n",
       "2  {'offer id': '2906b810c7d4411798c6938adc9daaa5'}     0  \n",
       "3  {'offer id': 'fafdcd668e3743c1bb461111dcafc2a4'}     0  \n",
       "4  {'offer id': '4d5c57ea9a6940dd891ad53e9dbe8da0'}     0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['offer received', 'offer viewed', 'transaction', 'offer completed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript.event.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data appears to be a little trickier because it is time-ordered, contains an event, and has a value. Depending on the event, it will be necessary to preprocess the 'value' column in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of unique people represented\n",
    "transcript['person'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are the same number of people in the transcript and the demographic data, which is encouraging. But a lot more preprocessing will be necessary before this dataset can yield any useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person    0\n",
       "event     0\n",
       "value     0\n",
       "time      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "transcript.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has no null values.\n",
    "\n",
    "I will need to expand the values into different columns based on the event in order to draw conclusions from the value column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=pd.concat([transcript, transcript['value'].apply(pd.Series)], axis=1)\n",
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though the `offer id` column ended up being duplicates so we have to clean it up further to ensure there is only one `offer id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column to ensure only one offer_id column\n",
    "transcript['offer_id_new']=np.where(transcript['offer id'].isnull() & transcript['offer_id'].notnull(),transcript['offer_id'],transcript['offer id'])\n",
    "\n",
    "#drop unnecessary offer_id columns\n",
    "transcript.drop(['offer id','offer_id'],axis=1,inplace=True)\n",
    "\n",
    "#rename offer_id column\n",
    "transcript.rename(columns={'offer_id_new':'offer_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the additional transcript columns can be used for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the preparation strategy for the model's data\n",
    "\n",
    "I went back to my goal before starting to preprocess the data for the model. I had to reconsider how I would clean and prepare the data for the models I intended to develop after conducting a preliminary analysis of the data.\n",
    "\n",
    "I must first define a 'effective' offer within the Starbucks app in order to pinpoint the primary factors that influence an offer's effectiveness. As a result, I investigated the datasets and their potential interactions further.\n",
    "\n",
    "I had to first investigate the types of events that each offer type contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "portfolio.rename(columns={'id':'offer_id'},inplace=True)\n",
    "\n",
    "#join transcript with offer type\n",
    "transcript=transcript.merge(portfolio,how='left',on='offer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.groupby(['event','offer_type'])['offer_type'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first objective in the data preprocessing stage is to develop a methodology to assign offer_id values to specific transactions, as the transactions events do not have an associated offer_id in the transcript event data.\n",
    "\n",
    "Additionally, it should be noted that BOGO and discount offers have an offer completed event recorded when offers are completed, while informational offers do not have this event associated with them. To define an effective offer, we can specify the following approach:\n",
    "\n",
    "For BOGO and discount offers, an effective offer is defined if the following events are recorded in the right sequence in time: offer received -> offer viewed -> transaction -> offer completed. This sequence of events indicates that the user received the offer, viewed it, made a transaction related to the offer, and then completed the offer.\n",
    "\n",
    "On the other hand, since there is no offer completed event associated with informational offers, we will have to define transactions as a conversion to an effective offer. In this case, an effective offer is defined if the following events are recorded in the right sequence in time: offer received -> offer viewed -> transaction. This sequence of events indicates that the user received the offer, viewed it, and made a transaction related to the offer, even though there was no offer completed event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### a. Assigning offer ids to transactions\n",
    "\n",
    "After outlining the aforementioned strategy, we must now investigate ways to associate offer_ids with particular transactions. one of the \n",
    "\n",
    "defining the following primary consumer groups is a consideration:\n",
    "\n",
    "**1. People who are influenced and successfully convert - effective offers:**\n",
    "\n",
    "    - `offer received` -> `offer viewed` -> `transaction` -> `offer completed` (BOGO/discount offers)\n",
    "    - `offer received` -> `offer viewed` -> `transaction` (informational offers - must be within validity period of offer)\n",
    "\n",
    "**2. People who received and viewed an offer but did not successfully convert - ineffective offers:**\n",
    "\n",
    "    - `offer received` -> `offer viewed`\n",
    "    \n",
    "**3. People who purchase/complete offers regardless of awareness of any offers:**\n",
    "    \n",
    "    - `transaction`\n",
    "    - `offer received` -> `transaction` -> `offer completed` -> `offer viewed`\n",
    "    - `transaction` -> `offer received` -> `offer completed` -> `offer viewed`\n",
    "    - `offer received` -> `transaction` -> `offer viewed` -> `offer completed`\n",
    "    - `offer received` -> `transaction` (informational offers)\n",
    "    - `offer received` -> `transaction` -> `offer viewed` (informational offers)\n",
    "\n",
    "**4. People who received offers but no action taken:**\n",
    "\n",
    "    - `offer received`\n",
    "    \n",
    "To identify ineffective offers, we need to check if there are users in group 2 who have both offer received and offer viewed events but no offer completed or transaction events, as this indicates that the offer did not lead to a conversion. We also need to separate group 2 from group 4, as group 4 users did not even have an offer viewed event.\n",
    "\n",
    "For users in group 3, conversions are only valid if they occur after the offer viewed event. If an offer completed or transaction event occurs before the offer viewed event, it is an invalid conversion. It is also possible that a transaction occurred before the offer viewed event, but the offer was still completed. In this case, it is still not a valid conversion.\n",
    "\n",
    "The target variable effective_offer can be defined as 1 for group 1 customers who completed an offer after receiving and viewing it. Users in group 2 can be defined as having an ineffective offer with effective_offer set to 0. Users in groups 3 and 4 can be deprioritized as they are either likely to purchase regardless of offers or are low priority customers.\n",
    "\n",
    "To assign the offer ID that influenced a transaction, we can sort the transcript dataset by person and time to ensure that each event occurs in sequence. We can then filter the dataset by offer viewed and transaction events to ensure that they occur in the correct order. Finally, we can use pandas' ffill() method to fill every transaction with the offer_id of the viewed offer, only if it occurs before the transaction. This will help us identify the offer that influenced a transaction in cases where there was no offer completed event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dropcolumns function as I will be doing this many times\n",
    "def drop_cols(drop_cols,df,inplace=False):\n",
    "    '''\n",
    "    inputs:\n",
    "    - drop_cols: list or string of column name to be dropped\n",
    "    - df: dataframe from which column should be dropped\n",
    "    - inplace: specify whether columns are dropped in place or not\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with dropped columns.\n",
    "       \n",
    "    '''\n",
    "    df=df.drop(columns=drop_cols,axis=1,inplace=inplace)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns to clean dataset\n",
    "transcript=drop_cols(['reward_x','reward_y'],transcript)\n",
    "#sort events by person and time\n",
    "transcript=transcript.sort_values(['person','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dataset for transactions that occur after an offer is viewed, forward fill offer ids by person\n",
    "offers_view_transacted=transcript[['time','offer_id','person','event']][(transcript['event']=='transaction') | (transcript['event']=='offer viewed')].groupby(['person','offer_id']).ffill()\n",
    "offers_view_transacted['offer_id']=offers_view_transacted['offer_id'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the above temporary dataset is just a subset of the `transcript` dataset, I can create a new dataset with the filled in offer ids for transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=transcript.merge(offers_view_transacted,how='left',on=['person','time','event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up dataset to unify multiple offer_id columns into one column\n",
    "transcript['offer_id']=np.where(transcript['offer_id_x'].isnull(),transcript['offer_id_y'],transcript['offer_id_x'])\n",
    "\n",
    "drop_cols(['offer_id_x','offer_id_y'],transcript,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge portfolio dataset again to get offer data for the transaction events\n",
    "transcript=transcript.merge(portfolio,how='left',on='offer_id')\n",
    "transcript['duration']=np.where(transcript['duration_x'].isnull(),transcript['duration_y'],transcript['duration_x'])\n",
    "drop_cols(['duration_x','offer_type_x','difficulty_x','channels_x','duration_y'],transcript,inplace=True);\n",
    "transcript.rename(columns={'channels_y':'channels','reward_y':'reward','difficulty_y':'difficulty','offer_type_y':'offer_type'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Flagging transactions and offers completed after offers viewed\n",
    "\n",
    "Identifying completed offers and transactions occurring after an offer is viewed is an important step in preparing the data for modeling and analysis. Once we have assigned an offer ID to a transaction occurring after an offer is viewed, we can use that information to subset the data according to the groups defined earlier and analyze each group separately.\n",
    "\n",
    "To flag the converted transactions and completed offers in our dataset, we need to ensure that the offer ID of the previous event is the same one. We have already tagged the offer IDs for all offer viewed, transaction, and offer completed events, so we can use the offer_id field to ensure that these events belong to the same offer.\n",
    "\n",
    "By checking that these events occur in the same event space and are in the correct sequence of time, we can be assured that a transaction and/or completed offer occurs only after an offer is viewed. To do this, we can create a new column to flag the previous offer ID using pandas' shift function. This function shifts the values of a column by a specified number of rows, allowing us to compare the offer ID of the current event with the offer ID of the previous event in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sample space of events consisting of offer viewed, transactions and offer completed\n",
    "offers_viewed_transactions_completed=transcript[(transcript['event']=='offer viewed') | (transcript['event']=='transaction') | (transcript['event']=='offer completed')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add extra column to flag the previous offer id\n",
    "offers_viewed_transactions_completed['offer_id_previous'] = offers_viewed_transactions_completed.groupby(['person','offer_id'])['offer_id'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag a completed transaction/offer completed as long as the previous offer id consists of events in the same sample space\n",
    "offers_viewed_transactions_completed['valid_completed']=np.where(offers_viewed_transactions_completed['offer_id_previous']==offers_viewed_transactions_completed['offer_id'],1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset `offers_viewed_transactions_completed` consists of all other possible events, all we need to do is to append the all `offers received` events in the `transactions_clean` dataset to ensure we have our complete dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get only offer received events\n",
    "offers_received=transcript[transcript['event']=='offer received'].copy()\n",
    "\n",
    "#ensure all columns are the same between datasets to be appended\n",
    "offers_received['offer_id_previous']=np.nan\n",
    "offers_received['valid_completed']=np.nan\n",
    "\n",
    "#append datasets to complete dataset of transactions\n",
    "transcript=offers_received.append(offers_viewed_transactions_completed)\n",
    "\n",
    "#sort values\n",
    "transcript=transcript.sort_values(['person','time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having assigned offer_ids for transactions for which an `offer viewed` event occurred prior, we can now revisit the four customer groups of unique person-offer_id pairs we are trying to analyse.\n",
    "\n",
    "Since we consider the conversion events of depending on offer type differently, we have to first separate the transcript into 3 different offer types, in order to accommodate for the different treatment in assigning the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to split into 3 offer types\n",
    "def split(offer_type,grp_df):\n",
    "    '''\n",
    "    Splits dataframe to groups of specified offer type.\n",
    "    \n",
    "    inputs:\n",
    "    - offer_type: specify offer type name in string format \n",
    "    - grp_df: original transcript dataframe to split on offer type\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing data of just offer type.\n",
    "       \n",
    "    '''\n",
    "    df=grp_df[grp_df['offer_type']==offer_type].copy()\n",
    "    return df\n",
    "\n",
    "#split transcript into 3 different offer types\n",
    "transcript_bogo=split('bogo',transcript)\n",
    "transcript_discount=split('discount',transcript)\n",
    "transcript_info=split('informational',transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each offer type, we can already successfully separate every unique person-offer_id in group 1 from the others using our `valid_completed` column. Since we have flagged all conversion events (`transaction` or `offer completed` event depending on offer type) occurring after an `offer viewed` event, we can be assured that whichever conversion events are flagged with `valid_completed=1` are at least within the first group (People who are influenced and successfully convert - effective offers).\n",
    "\n",
    "For BOGO and discount offers, we will only consider `offer completed` events as the conversion events, while we can consider `transaction` event as the conversion event for the informational offers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since will do this for both BOGO and discount, define function for repeated operation\n",
    "def grp1(df):\n",
    "    '''\n",
    "    Subsets dataframe to just group 1 members.\n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe \n",
    "\n",
    "    outputs:\n",
    "    - Returns dataframe containing transcript data of just group 1 users.\n",
    "       \n",
    "    '''\n",
    "    grp1=df[['person','offer_id']][(df['valid_completed']==1) & (df['event']=='offer completed')].groupby(['person','offer_id']).count().reset_index()\n",
    "    return grp1\n",
    "\n",
    "grp1_bogo=grp1(transcript_bogo)\n",
    "grp1_discount=grp1(transcript_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, for informational offers we will define group 1 later as there is an additional consideration we need to take into account for transactions - they need to occur within the validity period of an informational offer for us to consider them as effective offers.\n",
    "\n",
    "Now, we can look into separating group 2 and group 4 unique person-offer_ids for BOGO and discount offers as we just need to look at the subset of people with `offer received`, `offer viewed`, but no conversion events. We can also assume that every person who views an offer would have had an `offer received` event prior, so we can just take the whole group of people who received an offer and subset them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#again, we define a function as we will repeat this for 2 datasets - BOGO & discount\n",
    "def no_conv(df):\n",
    "    \n",
    "    '''\n",
    "    Takes in transcript dataframe of single offer type to check for people who converted vs people with just offer received events. \n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe of specific offer type \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing unqiue person-offer_id pairs with conversion events and offers received events, with indicator of each.\n",
    "    \n",
    "    Note: left_only indicator is just the offers received events, right_only is just conversion events\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #subset offer ids that have transactions or conversions by person and offer_id\n",
    "    conversion_ids=df[['person','offer_id']][(df['event']=='transaction') | (df['event']=='offer completed') ].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "    #check for unique person-offer_id pairs that consist of offers received \n",
    "    offers_received_only=df[['person','offer_id']][df['event']=='offer received'].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "    #create merged dataset to diffrentiate groups\n",
    "    check_merge=conversion_ids.merge(offers_received_only,how='right',on=['person','offer_id'],indicator=True)\n",
    "    return check_merge\n",
    "\n",
    "#check how many are in either group\n",
    "check_merge_bogo=no_conv(transcript_bogo)\n",
    "print('For BOGO offers:')\n",
    "print(check_merge_bogo.groupby(['_merge']).count())\n",
    "\n",
    "check_merge_discount=no_conv(transcript_discount)\n",
    "print('For Discount offers:')\n",
    "print(check_merge_discount.groupby(['_merge']).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are definitely a fair number of unique person-offer_id pairs that have `offer received` events, but no conversion events. These would be considered offers in group 2 and 4 within each offer type, according to our definition above. \n",
    "\n",
    "People with an `offer viewed` event in this subset are definitely in group 2, as we can assume everyone with an `offer viewed` event has an `offer received` event prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define group 2 & 4 function as will repeat this for BOGO and discount offers\n",
    "def grp_2_4(df):\n",
    "    \n",
    "    '''\n",
    "    Takes in output dataframe from no_conv function to split into group 2 and 4 customers.\n",
    "    \n",
    "    inputs:\n",
    "    - df: output dataframe from no_conv function\n",
    "    \n",
    "    outputs:\n",
    "    - Returns 2 dataframes containing unique person-offer_id pairs with dataframe containing only group2 customers first, followed by dataframe containing only group 4 customers. \n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #subset to check group 2 and 4\n",
    "    grp_2_4=df[df['_merge']=='right_only']\n",
    "\n",
    "    #remerge with transcript to get events\n",
    "    grp_2_4=grp_2_4.merge(transcript,how='left',on=['person','offer_id'])\n",
    "\n",
    "    #within this subset, separate people with offer viewed event, and people with offer received but no offer viewed\n",
    "    grp2=grp_2_4[['person','offer_id']][grp_2_4['event']=='offer viewed'].groupby(['person','offer_id']).count().reset_index()\n",
    "    \n",
    "    #remerge with full dataset and get remaining to get grp4\n",
    "    drop_cols('_merge',grp_2_4,inplace=True)\n",
    "    grp4=grp_2_4.merge(grp2[['person','offer_id']],how='left',indicator=True)\n",
    "    grp4=grp4[grp4['_merge']=='left_only'].copy()\n",
    "    \n",
    "    return grp2,grp4\n",
    "\n",
    "grp2_bogo,grp4_bogo=grp_2_4(check_merge_bogo)\n",
    "grp2_discount,grp4_discount=grp_2_4(check_merge_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 3 people are everyone in the converted ids who do not have an offer viewed prior - hence, they would be people with conversion events but no `offer viewed` event prior. For BOGO and discount offers, they would be people with `offer completed` events that have `valid_completed != 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp3(df):\n",
    "    '''\n",
    "    Takes in transcript dataframe of single offer type to check for people who converted vs people with just offer received events. \n",
    "    \n",
    "    inputs:\n",
    "    - df: original transcript dataframe of specific offer type \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing unqiue person-offer_id pairs with conversion events and offers received events, with indicator of each.\n",
    "       \n",
    "    '''\n",
    "    \n",
    "    #check all conversion events with invalid conversions\n",
    "    grp3=df[['person','offer_id']][(df['event']=='offer completed') & (df['valid_completed']!=1)].groupby(['person','offer_id']).count().reset_index()\n",
    "    return grp3\n",
    "\n",
    "grp3_bogo=grp3(transcript_bogo)\n",
    "grp3_discount=grp3(transcript_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have split our data into 4 different customer groups for the BOGO and discount offers. Next, we have to consider the effective and ineffective offers depending on the group type. As already elaborated above, any unique person-offer_id belonging to group 1 can be considered in our target variable `effective_offer=1` group.\n",
    "\n",
    "Meanwhile, group 2 is in our target variable `effective_offer=0` group. For customers in groups 3 and 4, I deprioritise them for model implementation, but will be doing some exploratory analysis on them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offers(grp1,grp2):\n",
    "    '''\n",
    "    inputs:\n",
    "    - grp1: dataframe containing group1 customer data \n",
    "    - grp2: dataframe containing group2 customer data\n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with labeled effective offer column\n",
    "    '''\n",
    "    #assign effective offer flag column\n",
    "    grp1['effective_offer']=1\n",
    "    grp2['effective_offer']=0\n",
    "\n",
    "    #append datasets together\n",
    "    offers=grp1.append(grp2,sort=False)\n",
    "    return offers\n",
    "\n",
    "offers_bogo=offers(grp1_bogo,grp2_bogo)\n",
    "offers_discount=offers(grp1_discount,grp2_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully prepared the target variables for our BOGO and discount datasets. \n",
    "\n",
    "Meanwhile, for informational offers in particular, before we can tag the effective offers column, there is one more consideration - the validity of the offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Considering duration/validity of offers in converted transactions from informational offers\n",
    "\n",
    "There is an additional rule to consider when considering an effective/converted transaction and offer. This applies for offers that are of type 'informational'. As already elaborated above, the reason why informational offers get a different treatment is because the conversion event is not an `offer completed` event, but a `transaction`.\n",
    "\n",
    "For informational offers, the `duration` of the offer can be considered to be the duration of the influence. Hence, we can make the assumption that an offer should only be considered effective if it is within the `duration` of the offer.\n",
    "\n",
    "Meanwhile, for BOGO and discount offers, we can assume that if there is a conversion/ `offer completed` event, it should be within duration as it would not make sense for an offer to be completed if an offer is past its validity period.\n",
    "\n",
    "As we saw in our data dictionary, the `time` of an event in the `transcript` data is in terms of hours. In order to ensure it is on the same scale as the `duration` of the offer, we have to convert it into days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time into days\n",
    "transcript_info['day_offer']=transcript_info['time']/24\n",
    "#drop unnecessary columns\n",
    "drop_cols(['time','value','offer_id_previous'],transcript_info,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort transactions to ensure all events occurring by person and offer\n",
    "transcript_info=transcript_info.sort_values(['person','day_offer','event','offer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the difference between two events using the `diff` function in pandas. We take the difference between the `transaction` and the `offer received` as the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get difference in time for informational offers\n",
    "transcript_info['diff_info']=transcript_info[(transcript_info['offer_type']=='informational') & ((transcript_info['event']=='offer received') | (transcript_info['event']=='transaction'))].groupby(['person','offer_id'])['day_offer'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column for flagging valid events\n",
    "transcript_info['valid_completed_duration']=np.nan\n",
    "\n",
    "#flag valid events if within duration\n",
    "transcript_info.loc[transcript_info['diff_info']<=transcript_info['duration'],'valid_completed_duration']=1\n",
    "\n",
    "#fill any missing values with 0 flag\n",
    "transcript_info['valid_completed_duration']=transcript_info['valid_completed_duration'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `valid_completed` and `valid_completed_duration` flag columns, we have 4 possible scenarios for an informational offer within the `transcript_info` dataset:\n",
    "\n",
    "|No.| valid_completed | valid_completed_duration | Scenario |\n",
    "|---| --- | --- | --- |\n",
    "|1| 1 | 0 | completed transaction after offer viewed event, but not within duration |\n",
    "|2| 0/null | 1 | completed transaction within duration, but with no offer viewed event prior |\n",
    "|3| 1 | 1 | completed transaction within duration, with offer viewed event - **an effective offer** |\n",
    "|4| 0//null | 0 | did not complete transaction within duration, no offer viewed event prior |\n",
    "\n",
    "Following the above scenarios, only Scenario 3 would be considered our label `effective_offers = 1` for informational offers (group 1 of customers).\n",
    "\n",
    "Meanwhile, Scenarios 1 and 2 can be considered to be actions that would put the customer into our Group 3 of customers - People who purchase/complete offers regardless of awareness of any offers. \n",
    "\n",
    "For customers in Scenario 1, even though according to our `valid_completed` flag, they had viewed an offer prior to the transaction, but it is not within the duration, thus they are not 'influenced' by the offer.\n",
    "\n",
    "Meanwhile for customers in Scenario 2, they are in Group 3 as they completed transactions without viewing an offer. \n",
    "\n",
    "Scenario 4 can be considered in group 4, as they only consist of transactions.\n",
    "\n",
    "We will need to separate those users in group 2 - those who may have received and viewed an offer, but no transactions after. We need to subset those where `effective_offer!=1` into groups 2,3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flag effective_offers where valid_completed=1 and valid_completed_duration=1\n",
    "transcript_info['effective_offer']=np.where(((transcript_info['valid_completed']==1) & (transcript_info['valid_completed_duration']==1)),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have flagged our effective offers, we can subset them into the 4 groups already outlined above. We can also filter this only for the `effective offers=1` events, as we only want the effective transactions influenced by an offer, not other transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate group 1 in transcript_into\n",
    "grp1_info=transcript_info[['person','offer_id']][transcript_info['effective_offer']==1].groupby(['person','offer_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the remaining people, we have to separate it out into groups 2 and 4. We can use similar steps to what we did with BOGO and Discount offers, since we don't have the duration consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate out group 2 of customers\n",
    "check_merge_info=no_conv(transcript_info)\n",
    "print('For informational offers:')\n",
    "print(check_merge_info.groupby(['_merge']).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp2_info,grp4_info=grp_2_4(check_merge_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For group 3, we have to consider those with conversions who do not have an offer viewed prior - hence, they would be people with conversion events but no offer viewed event prior. For informational offers, these would be `transaction`s in Scenario 1 and 2 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scenario 1\n",
    "grp3_1=transcript_info[['person','offer_id']][(transcript_info['event']=='transaction')&(transcript_info['valid_completed']!=1) & (transcript_info['valid_completed_duration']==1)].groupby(['person','offer_id']).count().reset_index()\n",
    "#scenario 2\n",
    "grp3_2=transcript_info[['person','offer_id']][(transcript_info['event']=='transaction')&(transcript_info['valid_completed']==1) & (transcript_info['valid_completed_duration']!=1)].groupby(['person','offer_id']).count().reset_index()\n",
    "grp3_info=grp3_1.append(grp3_2,sort=False)\n",
    "del grp3_1\n",
    "del grp3_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can append the datasets together to make the offers_info dataset, ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_info=offers(grp1_info,grp2_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have subset all our datasets into effective and ineffective offers depending on offer type, we can append the datasets accordingly into datasets for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Feature engineering\n",
    "\n",
    "Now we have to look back had to look into the features and see how to be creative in creating new features.\n",
    "\n",
    "#### d.i. `became_member_on` column to be engineered\n",
    "Recalling my preliminary data exploration steps, the `became_member_on` column were in date format. Hence in order to extract meaningful insights from that feature, we can convert it as a feature indicating tenure of membership. There could be some influence in how long someone has been a member, with whether he takes up an offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column for merging\n",
    "profile.rename(columns={'id':'person'},inplace=True)\n",
    "\n",
    "#create function to reuse for 3 datasets\n",
    "def member(df):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: original dataframe to transform became_member_on column  \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with became_member_on column transformed to be tenure in days\n",
    "    \n",
    "    '''\n",
    "    #merge to get user demographic profile\n",
    "    df=df.merge(profile,how='left',on='person')\n",
    "    \n",
    "    #convert became_member_on into member tenure\n",
    "    df['year']=pd.Series([int(str(x)[:4]) for x in df['became_member_on']])\n",
    "    df['month']=pd.Series([int(str(x)[-3]) for x in df['became_member_on']])\n",
    "    df['day']=pd.Series([int(str(x)[-2:]) for x in df['became_member_on']])\n",
    "    df=drop_cols('became_member_on',df)\n",
    "    df.loc[df['year'] == 2018, 'membership_tenure_days'] = (30*df['month'])+df['day']\n",
    "    df.loc[df['year'] != 2018, 'membership_tenure_days'] = ((2018-df['year'])*365)+(30*df['month'])+df['day']\n",
    "    df=drop_cols(['year','month','day'],df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "offers_bogo=member(offers_bogo)\n",
    "offers_discount=member(offers_discount)\n",
    "offers_info=member(offers_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.ii. Count of offers received\n",
    "As part of some further data exploration, I discovered that there could be multiple offers received per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of offers received per person\n",
    "offers_received = transcript[transcript['event'] == 'offer received'].groupby('person')['event'].count()\n",
    "offers_received.hist(density=True)\n",
    "plt.title('Distribution of Offers Received per Person')\n",
    "plt.xlabel('Number of Offers Received')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that the offer received per person in the transactional data could range from 1 to 6 offers received. I had the hypothesis that the frequency of offers received per person might result in more effective offers, so decided to engineer a feature `offer_received_cnt` to account for this frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get count of offers received per person, put into separate dataset\n",
    "df_offer_received_cnt=transcript[transcript['event']=='offer received'].groupby(['person','offer_id','time']).count()['event'].reset_index()\n",
    "\n",
    "#rename columns\n",
    "df_offer_received_cnt.rename(columns={'event':'offer_received_cnt'},inplace=True)\n",
    "\n",
    "#drop unnecessary columns\n",
    "drop_cols('time',df_offer_received_cnt,inplace=True)\n",
    "\n",
    "#ensure only unique person-offer_id pairs\n",
    "df_offer_received_cnt=df_offer_received_cnt.groupby(['person','offer_id']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.ii. Separating user behaviours by transactions\n",
    "\n",
    "I also wondered how many transactions were considered 'invalid' by my definition. Ordinarily, these would be the sum of transactions done by people not in group 1. The objective of offers are to drive purchases, so it would already be the case that users with high spend in their transactions would be flagged as `effective_offers`. \n",
    "\n",
    "We've already defined that there are people in groups 3 and 4, where they are separate pools of users who are loyal spenders, and already tend to purchase more, isolated from the the effect of offers. \n",
    "\n",
    "But for users in group 1 have a high amount of 'invalid spend' outside of the effect of offers, there might be some predictive power onto the effectiveness of offers; since a loyal user might have a higher tendency of taking up an offer.\n",
    "\n",
    "In my datasets, I had already separated the transactions who are conversions versus transactions who are just the users' normal purchasing behaviour. This is through the `valid_completed` column, where I checked if a transaction had an `offer viewed` event prior. \n",
    "\n",
    "In the cases where `valid_completed`=1, I had already included them in my effective offers flag for BOGO and Discount offers. However, for those transctions where `valid_completed`=0, I have not considered them, and this could be a potential feature to include, as a proxy for the 'baseline' level of spending for a user.\n",
    "\n",
    "The logic is to wonder if there is some baseline level of spending for users who are highly influenced by certain offers (in group 1), and group 2, and if there is some predictive power in this baseline level of 'invalid transactions' that can predict the propensity of a user to take up an offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter dataset by invalid transactions\n",
    "df_transactions_invalid=transcript[(transcript['event']=='transaction') & (transcript['valid_completed']==0)].groupby(['person','offer_id'])['amount'].sum().reset_index()\n",
    "df_transactions_invalid.rename(columns={'amount':'amount_invalid'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. iii. Time elapsed between offers received\n",
    "\n",
    "I also wanted to include time as a potential feature into my dataset, but since the transactional data starts from time=0, I suspected it would not have been of much predictive power without some feature engineering. I had the hypothesis that if there were multiple offers received per person within a certain time period, there might be some predictive power in the time elapsed between offers received. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time into days\n",
    "transcript['day_offer']=transcript['time']/24\n",
    "#drop unnecessary columns\n",
    "drop_cols(['time'],transcript,inplace=True);\n",
    "\n",
    "#find time elapsed between offers received\n",
    "transcript['time_elapsed_offers']=transcript[transcript['event']=='offer received'].groupby(['person','offer_id'])['day_offer'].diff()\n",
    "\n",
    "#fill missing values with 0, as if someone does not receive an offer or is receiving an offer for the first time, there is no time elapsed\n",
    "transcript['time_elapsed_offers']=transcript['time_elapsed_offers'].fillna(value=0)\n",
    "\n",
    "#create temporary dataset\n",
    "df_time_elapsed=transcript.groupby(['person','offer_id'])['time_elapsed_offers'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Preparing data for implementation\n",
    "\n",
    "Now we can finally begin with preparing the data for modeling. \n",
    "\n",
    "To do this, there are some additional preparation steps for each dataset. Recalling our initial preliminary data exploration, there are some steps to prepare the data:\n",
    "\n",
    "a. Merge with temporary datasets created above to include engineered features\n",
    "\n",
    "b. Drop missing values in `gender` column for demographic data; convert gender into dummy variables\n",
    "\n",
    "c. Separate the `channel` column into categorical variables\n",
    "\n",
    "d. Treatment of duplicate records\n",
    "\n",
    "#### f. ii.a. Merge with temporary datasets created above to include engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to get offers received count and invalid amount transacted \n",
    "offers_bogo=offers_bogo.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "offers_bogo=offers_bogo.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.ii.b. Drop missing values in gender column for demographic data\n",
    "\n",
    "Now, we need to check whether dropping the missing values will result in a significant loss in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check % of missing values in dataset\n",
    "(offers_bogo.isnull().sum()/len(offers_bogo)*100).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the missing values are quite extensive especially for the `amount_invalid` column. It is debatable whether this column `amount_invalid` would be useful to include in the model. Since it is so 'sparse' for BOGO offers, it might not have much information after all. I plan to assess this feature again later during the model implementation phase. For now, I decided to fill the missing `amount_invalid` column with 0 as it could represent that only 3% of the overall users tend to purchase without offers; the other 97% would only purchase with awareness of an ongoing offer. \n",
    "\n",
    "Meanwhile, we had already conducted the analysis above on the `income` and  `gender` columns, which I choose to drop as they are not useful when they are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing values for amount_invalid with 0\n",
    "offers_bogo['amount_invalid']=offers_bogo['amount_invalid'].fillna(value=0)\n",
    "\n",
    "#drop income and gender null rows\n",
    "offers_bogo.dropna(inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.ii.c. Separate the channel column into categorical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foresee need to reuse function so create rename function\n",
    "def rename(col_name,df):\n",
    "    df[col_name]=np.where(df[col_name]==col_name,1,0)\n",
    "    return df\n",
    "\n",
    "#foresee need to reuse dummy variable encoding function\n",
    "def dummy(df,col):\n",
    "    df=pd.concat([df[:],pd.get_dummies(df[col],prefix=col)],axis=1)\n",
    "    df=drop_cols(col,df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with portfolio to get offer details\n",
    "offers_bogo=offers_bogo.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "#convert channels into categorical variables\n",
    "channels = offers_bogo['channels'].apply(pd.Series)\n",
    "channels = channels.rename(columns={0:'web',1:'email',2:'mobile',3:'social'})\n",
    "offers_bogo=pd.concat([offers_bogo[:], channels[:]], axis=1)\n",
    "rename('web',offers_bogo)\n",
    "rename('email',offers_bogo)\n",
    "rename('mobile',offers_bogo)\n",
    "rename('social',offers_bogo)\n",
    "offers_bogo=drop_cols('channels',offers_bogo)\n",
    "\n",
    "#convert gender into categorical variables\n",
    "offers_bogo=dummy(offers_bogo,'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to repeat these steps for `offers_discount`, I created a function containing all the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_offers_df(df):\n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    - df: original dataframe for modeling \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe containing engineered features, filled missing values and cleaned and transformed variables (channel and gender)\n",
    "       \n",
    "    '''\n",
    "    #merge to get engineered features \n",
    "    df=df.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "    df=df.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])\n",
    "    \n",
    "    #fill missing values for amount_invalid with 0\n",
    "    df['amount_invalid']=df['amount_invalid'].fillna(value=0)\n",
    "    \n",
    "    #drop income and gender null rows\n",
    "    df.dropna(inplace=True);\n",
    "    \n",
    "    #merge with portfolio to get offer details\n",
    "    df=df.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "    #convert channels into categorical variables\n",
    "    channels = df['channels'].apply(pd.Series)\n",
    "    channels = channels.rename(columns={0:'web',1:'email',2:'mobile',3:'social'})\n",
    "    df=pd.concat([df[:], channels[:]], axis=1)\n",
    "    rename('web',df)\n",
    "    rename('email',df)\n",
    "    rename('mobile',df)\n",
    "    rename('social',df)\n",
    "    df=drop_cols('channels',df)\n",
    "    \n",
    "    #convert gender column into dummy variables\n",
    "    df=dummy(df,'gender')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for offer_discounts\n",
    "offers_discount=prep_offers_df(offers_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `offers_info` dataset, a slightly different treatment needs to be done as the `channels` column contains a different order of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with portfolio to get offer details\n",
    "offers_info=offers_info.merge(portfolio,how='left',on='offer_id')\n",
    "\n",
    "#reset index for offers_info\n",
    "offers_info=drop_cols('index',offers_info.reset_index())\n",
    "\n",
    "#expand channel column into categorical variables\n",
    "def channel_col(name,df=offers_info):\n",
    "    '''\n",
    "    inputs:\n",
    "    - name: name of channel column to be transformed \n",
    "    - df: dataframe \n",
    "    \n",
    "    outputs:\n",
    "    - offer_info dataframe with channel column transformed\n",
    "    \n",
    "    '''\n",
    "    df[name]= np.nan\n",
    "    df.loc[pd.Series([name in df['channels'][x] for x in range(len(df['channels']))]),name]=1\n",
    "    df[name]=df[name].fillna(value=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_col('web')\n",
    "channel_col('email')\n",
    "channel_col('mobile')\n",
    "channel_col('social');\n",
    "\n",
    "drop_cols('channels',offers_info,inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repurpose function for offers_info\n",
    "def prep_offers_df(df):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: dataframe to be transformed \n",
    "    \n",
    "    outputs:\n",
    "    - Returns dataframe with engineered features and filled missing values, with transformed gender column.\n",
    "       \n",
    "    '''\n",
    "    #merge to get engineered features \n",
    "    df=df.merge(df_offer_received_cnt[['person','offer_id','offer_received_cnt']],how='left',on=['person','offer_id'])\n",
    "    df=df.merge(df_transactions_invalid[['person','offer_id','amount_invalid']],how='left',on=['person','offer_id'])\n",
    "\n",
    "    #fill missing values for amount_invalid and offer_received_cnt with 0\n",
    "    df['amount_invalid']=df['amount_invalid'].fillna(value=0)\n",
    "\n",
    "    #drop income and gender null rows\n",
    "    df.dropna(inplace=True);\n",
    "    \n",
    "    #convert gender column into dummy variables\n",
    "    df=dummy(df,'gender')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_info=prep_offers_df(offers_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offers_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f.i. Treatment of duplicate records\n",
    "\n",
    "Since we have subset the data cleanly according to unique person-offer_id pairs by group, we should not have any duplicate records. But just in case, we check to make sure we have no duplicate records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check multiple records for each person and offer ids for the target variable\n",
    "print((offers_bogo.groupby(['person','offer_id','effective_offer']).size()>1).sum())\n",
    "print((offers_discount.groupby(['person','offer_id','effective_offer']).size()>1).sum())\n",
    "print((offers_info.groupby(['person','offer_id','effective_offer']).size()>1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Now that the datasets are ready, we can proceed to implementing the model. Revisiting our objective, we wanted to analyse the drivers of an effective offer, with the target variable being `effective_offer`.\n",
    "\n",
    "Since we have 3 offer types, there are thus 3 different models to be built. Since we are predicting whether an offer would be effective or not, this is effectively a binary classification supervised learning model.\n",
    "\n",
    "I decided to compare the performance of a simple decision tree classifier model as a baseline model, with an ensemble random forest classifier model. Reason why I selected a decision tree as the baseline model is because I wanted to prioritise the interpretability of the model. Going back to the objective, since we intend to analyse the feature importance to determine the drivers of an effective offer, a decision tree would provide good interpretability for us to analyse.\n",
    "\n",
    "Meanwhile, I also selected random forest as an alternate model to compare the baseline model is as an improvement over simple ensemble bagging of decision trees, in order to drive towards a high accuracy in training the model. \n",
    "\n",
    "Before we can proceed, we have to make sure that the classes we are predicting for are balanced in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for class balance in datasets\n",
    "print(offers_bogo[['person','effective_offer']].groupby('effective_offer').count()/len(offers_bogo))\n",
    "print(offers_discount[['person','effective_offer']].groupby('effective_offer').count()/len(offers_discount))\n",
    "print((offers_info[['person','effective_offer']].groupby('effective_offer').count()/len(offers_info)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classes are quite uneven for all three offer types, but not too imbalanced such that it would pose a problem. Hence, we can proceed to implement the models.\n",
    "\n",
    "A note on model evaluation and validation; since the classes for the all 3 models are imbalanced, I decided to implement both accuracy and f1 score as the model evaluation metric. F1 score provides a better sense of model performance compared to purely accuracy as takes both false positives and false negatives in the calculation. With an uneven class distribution, F1 may usually be more useful than accuracy. \n",
    "\n",
    "It is worth noting in this case that the F1 score is based on the harmonic mean of precision and recall, and focuses on positive cases. For the Starbucks app here, it would be fine as we would prioritise more on whether offers are effective, and less focus on why offers are ineffective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model Implementation\n",
    "\n",
    "Revisiting our objective, we are creating 3 models to predict the effectiveness of an offer within each type, depending on offer attributes and user demographics.\n",
    "\n",
    "First, we have to define our target and features variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df,drop_cols_prep):\n",
    "    '''\n",
    "    inputs:\n",
    "    - df: prepared dataframe for modeling \n",
    "    \n",
    "    outputs:\n",
    "    - Returns 2 dataframes - features and target dataframes\n",
    "    '''\n",
    "    # Split the data into features and target label\n",
    "    target = df['effective_offer']\n",
    "    features = drop_cols(drop_cols_prep,df)\n",
    "    return features,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I split the data into training and test sets. Since the features of my data are all on different scales, I also apply a scaler to ensure my data will all be on the same scale for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare model pipeline\n",
    "def model_pipeline(features,target):\n",
    "    '''\n",
    "    inputs:\n",
    "    - features & target dataframe \n",
    "    \n",
    "    outputs:\n",
    "    - Splits features and target dataframe to train and test sets, performs feature scaling on both datasets.\n",
    "    - Outputs X_train, X_test, y_train and y_test dataframes\n",
    "    '''\n",
    "    \n",
    "    #split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=42)\n",
    "\n",
    "    #fit and transform scaling on training data\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "\n",
    "    #scale test data\n",
    "    X_test=scaler.transform(X_test)\n",
    "    return X_train,X_test,y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am defining the functions here to run my model as I plan to implement 3 different models; hence it would be easier to implement repeatedly. In this function, I define the model scores - F1 score and accuracy, as well as the error (mean squared error). As elaborated above, I plan to compare the F1 score with the accuracy score as a better indication of model performance, especially since the classes for the BOGO and discount offers are uneven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: review_scores_rating training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: review_scores_rating testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    #Fit the learner to the training data and get training time\n",
    "    start = time() \n",
    "    learner = learner.fit(X_train, y_train)\n",
    "    end = time() \n",
    "    results['train_time'] = end-start\n",
    "    \n",
    "    # Get predictions on the test set(X_test), then get predictions on first 300 training samples\n",
    "    start = time() \n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() \n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    #add training accuracy to results\n",
    "    results['training_score']=learner.score(X_train,y_train)\n",
    "    \n",
    "    #add testing accuracy to results\n",
    "    results['testing_score']=learner.score(X_test,y_test)\n",
    "     \n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, len(y_train)))\n",
    "    print(\"MSE_train: %.4f\" % mean_squared_error(y_train,predictions_train))\n",
    "    print(\"MSE_test: %.4f\" % mean_squared_error(y_test,predictions_test))\n",
    "    print(\"Training accuracy:%.4f\" % results['training_score'])\n",
    "    print(\"Test accuracy:%.4f\" % results['testing_score'])\n",
    "    print(classification_report(y_test, predictions_test,digits=4))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(clf1,clf2,name):\n",
    "    '''\n",
    "    inputs:\n",
    "    - clf1: first classifier model\n",
    "    - clf2: 2nd classifier model for comparison\n",
    "    - name: name of models for comparison\n",
    "    \n",
    "    outputs:\n",
    "    - Dataframe of results from model training and prediction\n",
    "    '''\n",
    "    \n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [clf1, clf2]:\n",
    "        clf_name = clf.__class__.__name__ + '_' +name\n",
    "        results[clf_name] = {}\n",
    "        results[clf_name]= train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.i. BOGO offers model\n",
    "\n",
    "First we try to build the BOGO offers model. I initialize the models with some randomly chosen parameters to check the initial performance. If performance needs to be improved further, I will attempt Grid Search to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - baseline is DT model, bogo_1 model is RF model\n",
    "baseline = DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=2,min_samples_split=90,min_samples_leaf=50)\n",
    "bogo_1 = RandomForestClassifier(random_state=2,max_depth= 11, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=20)\n",
    "\n",
    "results=run_model(baseline,bogo_1,'bogo_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for Random Forest Classifier (RF) model actually ends up outperforming the Decision Tree Classifier (DT) model slightly, but overall the performance for both models is about the same (82.14% vs 81.77% respectively in terms of accuracy). \n",
    "Accuracy for a first attempt is quite good, more than 80%. I will try to tune the model further to get a better accuracy.\n",
    "\n",
    "However, in terms of the F1 score, both models are below 80%, with the Random Forest model performing worse compared to the Decision Tree Classifier, with 75.91% vs. 79.63%. To analyse this, we have to refer to the formula for Precision, Recall and F1 score:\n",
    "\n",
    "**Recall or Sensitivity or TPR (True Positive Rate):** \n",
    "\n",
    "According to sklearn documentation, the recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Number of items correctly identified as positive out of total true positives: True Positives /(True Positives +False Negatives)\n",
    "\n",
    "**Precision:** \n",
    "\n",
    "According to the sklearn documentation, it is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "Number of items correctly identified as positive out of total items identified as positive: True Positives /(True Positives + False Positives)\n",
    "\n",
    "\n",
    "**F1 Score:** \n",
    "\n",
    "Since my F-beta score is F1 with beta=1, I am weighting recall and precision as equally important.\n",
    "\n",
    "The formula is given by the harmonic mean of precision and recall:  F1 = 2*Precision*Recall/(Precision + Recall)\n",
    "\n",
    "We can see that the F1 scores for DT outperformed RF slightly, but both are lower than the accuracy. This would indicate that DT model is doing slightly better compared to RF at not misclassifying negative events as positive (meaning, misclassifying people on which offers are ineffective, as people on which offers would be effective).  \n",
    "\n",
    "The difference in F1 score vs accuracy indicate that there could are instances where both models are falsely classifying negatives as positives, likely due to the imbalance of classes. But the overall higher recall/accuracy compared to F1 score indicates that the model is predicting the positive case (i.e. where an offer is effective) more accurately compared to predicting the negative cases (i.e. where an offer is ineffective), which is expected given the uneven classes..\n",
    "\n",
    "However, revisiting our use case, we are perhaps not as concerned with these misclassification since we don't mind sending people more offers than they would have liked; we would rather not miss anyone on which an offer would have been effective.\n",
    "\n",
    "Given this case, I will still go with the RF model.\n",
    "\n",
    "Since I aim to analyse the drivers of an effective offer, I will check the feature importances for the models after I have selected the best model from refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.ii. Discount offers model\n",
    "\n",
    "I repeat the same steps above but with my offer_discounts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_1 = RandomForestClassifier(random_state=2,max_depth= 20, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=10)\n",
    "results=pd.concat([results[:],run_model(baseline,discount_1,'discount_1')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the Random Forest Classifier model also has a better performance compared to the Decision Tree Classifier in terms of accuracy (87.23% vs 86.72%), and the F1 score is also lower (81.43% vs 82.87%). \n",
    "\n",
    "The F1 score for these models are lower overall compared to the Accuracy score. This could be an indication that there are some instances where both models are classifying the negative cases (effective_offer = 0) falsely. Again, I am not too bothered by this as I am more concerned with the model predicting positive cases accurately, so would rather go with a higher accuracy model where F1 score for cases `effective_offer=1` is higher, for which our RF classifier has better performance (0.9317 vs 0.9280)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.iii.  Informational offers model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_1 = RandomForestClassifier(random_state=5,criterion='gini',max_depth= 20, max_features= 'auto',min_samples_split= 10,n_estimators=20,min_samples_leaf=10)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_1,'info_1')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance for these models are worse compared to the other 2 datasets, with accuracy below 80% for both models, but RF model still performing better. The F1 score is also worse, at 67.54% RF Classifier, worse than the DT model at 68.66%.\n",
    "\n",
    "One potential reason for the worse performance is perhaps due to the fact that I had the key assumption to assign the conversion events to be transactions that only occur after an offer is viewed and within the specified duration; I might have missed out on some valuable information by removing those transactions that occur regardless. We can see this from how the overall sample dataset is smaller (about half) the datasets for the other 2 offers, with only about 5K samples compared to about 10K for both BOGO and discount respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Refinement\n",
    "\n",
    "In refining the model, I will first try parameter tuning for the 3 RF models, before expreimenting with removing or adding features to improve model performance. \n",
    "\n",
    "Since I will be comparing the models based on testing score repeatedly, I built a function to find the best RF model results based on refinement depending on offer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to find best model results for each offer type\n",
    "def best_model(offer_type):\n",
    "    '''\n",
    "    input:\n",
    "    - offer_type: string of offer type name\n",
    "    output:\n",
    "    - dataframe containing results of best model so far\n",
    "    \n",
    "    '''\n",
    "    print('For ' + offer_type + ' RF model:')\n",
    "    return results.transpose()[results.transpose()['testing_score']==results.transpose()[results.transpose().index.str.contains(\"RandomForestClassifier_\"+offer_type)]['testing_score'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. i. Grid Search to discover optimal parameters\n",
    "I decided to do GridSearch to determine what would be the optimal parameters for the model.\n",
    "\n",
    "For all three offers, the Random Forest model had relatively good performance, so I used Grid Search on this to determine the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Grid Search function\n",
    "def rand_forest_param_selection(X,y):\n",
    "    '''\n",
    "    input:\n",
    "    - X,y: training datasets for X and y\n",
    "    output:\n",
    "    - dictionary with best parameters for random forest model\n",
    "    '''\n",
    "    \n",
    "    param_grid={'max_features': ['auto', 'sqrt'],\n",
    "                'max_depth' : [5,10,15,20],\n",
    "                'n_estimators': [10,20,25,30,40,50],\n",
    "                'min_samples_split': [2, 10, 20],\n",
    "                'min_samples_leaf': [2, 10,15, 20],\n",
    "                }\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=2), param_grid)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define BOGO dataset\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the optimal parameters for the BOGO model, I run my model again with the new parameters, keeping the DecisionTree baseline model with the same parameters as comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "bogo_2 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_2,'bogo_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_1','RandomForestClassifier_bogo_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the RF model increased slightly - from 82.14% to 82.51%, and the F1 score increased from 75.91% to 77.64%. This is a good performance increase but minimal, which indicates that perhaps there's not much that can be done to improve the performance of the model with parameter tuning. \n",
    "\n",
    "So I will have to explore other avenues with the features to improve the performance of the model further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define discount dataset\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "# run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_2 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_2,'discount_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_1','RandomForestClassifier_discount_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model increaased slightly, from 87.23% to 87.47%, and the F1 score improved from 81.43% to 82.06%. The good thing is that now both the accuracy and the F1 score for the RF model is better than the DT model. \n",
    "\n",
    "But because the increase was minimal, again we can conclude that tuning the parameters won't really improve the performance of the model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define info dataset\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#run Grid Search - commented out because takes to long to run, but have put in selected params in model\n",
    "# rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_2 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_2,'info_2')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_1','RandomForestClassifier_info_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see some improvement in accuracy for RF model, from 75.09% to 75.30%, and slight increase in F1 score from 67.54% to 67.78%. This improvement is minimal,so we look into improving the feature selection of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.ii Removing sparse features e.g. amount_invalid\n",
    "\n",
    "In terms of feature selection, I wanted to try and see if removing the amount_invalid variable, which we had noted as being sparse, hence may not be useful in predicting the effectiveness of offers, would help.\n",
    "\n",
    "I removed the feature from my data prep and retrained the model using the same optimal parameters found via GridSearch, with the DT model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add amount_invalid variable to drop_cols_prep list\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid']\n",
    "\n",
    "#train BOGO model\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "bogo_3 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_3,'bogo_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_2','RandomForestClassifier_bogo_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model accuracy and F1 score did improve, so I will leave the amount_invalid feature out of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train discount model\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "discount_3 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_3,'discount_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_2','RandomForestClassifier_discount_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of the model actually increased while F1 model remained the same. In this case, I will also remove the amount_invalid feature for the discount model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train info model\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "info_3 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_3,'info_3')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_2','RandomForestClassifier_info_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and F1 score of the model actually decreased here for info model, so I will also keep the feature in. This is expected since the model had already a worse performance compared to the other 2 models, so the model is slightly underfitting compared to the others. Hence the model needs more features to learn to predict better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. iii. Dropping one level of dummy variables/one-hot encoding\n",
    "\n",
    "There is a debate when using tree models and using regression models when it comes to one hot encoding. For regression classification models (e.g. logistic regression, we should typically remove one level of the variable in order to prevent multicollinearity between variables. Typically, we should not run into this issue with tree-based models like the ones I am using here. \n",
    "\n",
    "However, there is some debate as to whether one should do it or not. According to some articles (like here: https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/), it is generally not advisable to encode categorical variables as they would generate sparse matrices, resulting in:\n",
    "\n",
    "1. The resulting sparsity virtually ensures that continuous variables are assigned higher feature importance.\n",
    "2. A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.\n",
    "\n",
    "In scikitlearn implementations of RF and DT, one has to encode the variables. So I decided to test my model performance if I were to drop one level of my categorical variables (in my data - the channel variables and the gender variables), just to reduce the sparsity and noise in the data for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add one level of dummy variables to drop column \n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid','social','gender_O']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "bogo_4 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,bogo_4,'bogo_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_bogo_3','RandomForestClassifier_bogo_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for BOGO offer type\n",
    "best_model('bogo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of this model was not as good as previous model - hence I will keep alll levels of variables in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "discount_4 = RandomForestClassifier(random_state=2,max_depth= 10, max_features= 'auto',min_samples_split= 20,n_estimators=30,min_samples_leaf=2)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,discount_4,'discount_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_discount_3','RandomForestClassifier_discount_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for discount offer type\n",
    "best_model('discount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, accuracy of model was not as good, and minimal improvement. Hence I will keep all levels in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model - reuse best performing model - \n",
    "info_4 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_4,'info_4')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_3','RandomForestClassifier_info_4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we have seen that there is not much improvement in model performance just by reducing one level of categorical features. I am quite satisfied with the performance of the BOGO and discount models, but want to explore if I can improve the performance of the info model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. iv. Using polynomial features\n",
    "\n",
    "Since a low accuracy score for the info model is likely due to the model underfitting, I decided to attempt if transforming the features further might improve model performance.\n",
    "\n",
    "I tweaked my model_pipeline function to include the polynomial features transformation to my features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare model pipeline\n",
    "def model_pipeline_poly(features,target,poly_feat=0):\n",
    "    '''\n",
    "    input:\n",
    "    - features & target dataframes\n",
    "    - poly_feat: number of degrees to transform polynomial features\n",
    "    \n",
    "    output:\n",
    "    - X_train, X_test, y_train, y_test dataframes\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features,target, \n",
    "                                                        test_size=0.20, \n",
    "                                                        random_state=42)\n",
    "    #fit and transform training data\n",
    "    poly = PolynomialFeatures(poly_feat)\n",
    "    X_train_poly=poly.fit_transform(X_train)\n",
    "    \n",
    "    #transform test data\n",
    "    X_test_poly=poly.transform(X_test)\n",
    "    \n",
    "    #fit and transform scaling on training data\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train_poly)\n",
    "\n",
    "    #scale test data\n",
    "    X_test=scaler.transform(X_test_poly)\n",
    "    return X_train,X_test,y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep amount_invalid in offers_info dataset\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_info,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline_poly(features,target,2)\n",
    "\n",
    "#Initialize the model\n",
    "info_5 = RandomForestClassifier(random_state=2,max_depth= 15, max_features= 'auto',min_samples_split= 2,n_estimators=20,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,info_5,'info_5')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['RandomForestClassifier_info_2','RandomForestClassifier_info_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best model so far for info offer type\n",
    "best_model('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performance actually decreased slightly for the RF model. Hence it would perhaps be a better idea to just keep the model as is. A maximum accuracy of 75.30% is acceptable for the info offers, even though it is not as high as the BOGO or discount offers. After all, we already included some assumptions for the 'influence' of the offer based on the duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[['training_score','testing_score'],['RandomForestClassifier_info_1','RandomForestClassifier_info_2','RandomForestClassifier_info_3','RandomForestClassifier_info_4','RandomForestClassifier_info_5']].transpose().plot.line()\n",
    "plt.title('Training and Testing score for RF info models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note however, we can above actually see the model is performing better in the training accuracy as we add more variables for each model via polynomial features and removing the amount_invalid feature. It is just that the testing accuracy was reducing, and we can see this is due to overfitting.\n",
    "\n",
    "I can improve the accuracy and performance of the info model further by using RF info model 5, but adding more data, as we already noted the dataset for the `offers_info` dataset is half the size of the BOGO and discount datasets. Hence, ultimately with more data and with performance tuning, removing unnecessary variables and feature transformation, with more data I could have ultimately got the performance of the model perhaps above 80%.\n",
    "\n",
    "**b.iv. Discussion on best models and feature importances:**\n",
    "\n",
    "Now that I am done with refining the 3 models, we can check the results for our best models for all 3 and check the feature importances to see the top drivers of effectiveness of offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get best model overall for bogo,discount and info offers\n",
    "best_model('bogo').append([best_model('discount'),best_model('info')]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that the top performing models are the 3rd model (with GridSearch to find optimal model parameters and removing amount_invalid column) for predicting effectiveness of BOGO and discount offers, whereas the best performing model for informational offers was just after performing GridSearch to find the optimal parameters.\n",
    "\n",
    "In order to find the most influential drivers of an effective offer, we can check the feature importances of our best models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show feature importance\n",
    "#BOGO 3 model\n",
    "#prepare data same as BOGO 3 state\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type','amount_invalid']\n",
    "features,target=data_prep(offers_bogo,drop_cols_prep)\n",
    "\n",
    "feature_importances = pd.DataFrame(bogo_3.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best BOGO model feature importance')\n",
    "plt.show()\n",
    "\n",
    "#discount 3 model\n",
    "feature_importances = pd.DataFrame(discount_3.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best discount model feature importance')\n",
    "plt.show()\n",
    "\n",
    "#info_2 model\n",
    "#prepare data similar to info_2 state\n",
    "drop_cols_prep=['person','offer_id','effective_offer','offer_type']\n",
    "features,target=data_prep(offers_discount,drop_cols_prep)\n",
    "#print feature importance\n",
    "feature_importances = pd.DataFrame(info_2.feature_importances_,\n",
    "                                   index = features.columns,\n",
    "                                    columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances.plot.bar()\n",
    "plt.title('Best info model feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking on the feature importance to analyse the main drivers of an effective offer, we can see that the most important driver of effective offers across all three are the tenure of membership. However, the 2nd most important feature is different for each of the three models.\n",
    "\n",
    "For a BOGO offer, the membership tenure is the most important feature, and the other variables are a lot smaller in proportions. Income, age and offer_received_cnt are the 2nd, 3rd and 4th most important features, but their proportions are very small.\n",
    "\n",
    "For a discount offer, after the membership tenure, age and income are the next most important variables. But it is still very small in proportions.\n",
    "\n",
    "The feature importances for the informational offer models are more distributed compared to the BOGO and discount models, with income being the 2nd most important feature. Age is the third and mobile channel interestingly being the 4th."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Exploration on users in Groups 3 and 4 - People who purchase regardless of viewing any offers\n",
    "\n",
    "We had earlier delineated those in groups 3 and 4 as people who would purchase regardless of viewing any offers. Now we can do some exploratory analyses to see what kind of demographic this group of users consist of.\n",
    "\n",
    "**c.i. Data Preparation:**\n",
    "\n",
    "It would be interesting to see how people in groups 3 and 4 contrast with people in groups 1 and 2, so I decided to compare between all 3.\n",
    "\n",
    "First, I need to append the data from all groups from the three offer types together, then compare the characteristics of each group via visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append datasets together\n",
    "\n",
    "#grp 3+4\n",
    "grp3_4=grp3_bogo.append(grp3_discount,sort=False)\n",
    "grp3_4=grp3_4.append(grp3_info,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_bogo,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_discount,sort=False)\n",
    "grp3_4=grp3_4.append(grp4_info,sort=False)\n",
    "\n",
    "#grp1\n",
    "grp1_all=grp1_bogo.append(grp1_discount,sort=False)\n",
    "grp1_all=grp1_all.append(grp1_info,sort=False)\n",
    "\n",
    "#grp2\n",
    "grp2_all=grp2_bogo.append(grp2_discount,sort=False)\n",
    "grp2_all=grp2_all.append(grp2_info,sort=False)\n",
    "\n",
    "#get unique person-offer_id pairs\n",
    "grp3_4=grp3_4[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "grp1_all=grp1_all[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "grp2_all=grp2_all[['person','offer_id']].groupby(['person','offer_id']).count().reset_index()\n",
    "\n",
    "#get membership_tenure_days\n",
    "grp3_4=member(grp3_4)\n",
    "grp1_all=member(grp1_all)\n",
    "grp2_all=member(grp2_all)\n",
    "\n",
    "#merge with transcript to check transaction amount\n",
    "grp3_4=grp3_4.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')\n",
    "grp1_all=grp1_all.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')\n",
    "grp2_all=grp2_all.merge(transcript[['person','offer_id','amount']].groupby(['person','offer_id']).sum(),on=['person','offer_id'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also clean the dataset of null values, similar to the preparation of the datasets above for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values\n",
    "print(\"For grp 3 and 4:\")\n",
    "print((grp3_4.isnull().sum()/len(grp3_4))*100)\n",
    "\n",
    "#drop null values\n",
    "grp3_4=grp3_4.dropna()\n",
    "\n",
    "#check null values\n",
    "print(\"For grp 1:\")\n",
    "print((grp1_all.isnull().sum()/len(grp1_all))*100)\n",
    "\n",
    "#drop null values\n",
    "grp1_all=grp1_all.dropna()\n",
    "\n",
    "#check null values\n",
    "print(\"For grp 2:\")\n",
    "print((grp2_all.isnull().sum()/len(grp2_all))*100)\n",
    "\n",
    "#drop null values\n",
    "grp2_all=grp2_all.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check size of groups\n",
    "print(\"Size of group 1: \"+ str(len(grp1_all['person'])))\n",
    "print(\"Size of group 3+4: \"+ str(len(grp3_4['person'])))\n",
    "print(\"Size of group 2: \"+ str(len(grp2_all['person'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the sizes of the 3 groups, we can see that group 1 is the largest, while group 2 is the smallest, which is unsurprising as we had seen that the classes in our datasets were imbalanced in favour of positive classes (i.e. `effective_offers=1`). Meanwhile for people in groups 3 and 4 there are quite a significant number of people as well, larger than the number of people in group 2.\n",
    "\n",
    "**c.ii. Exploration of demographic characteristics:**\n",
    "\n",
    "Meanwhile, in order to effectively compare between the groups, I created a function to efficiently visualize the groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for plotting multiple histograms overlaying the 3 groups\n",
    "def plot_hist(variable,bins=None):\n",
    "    plt.hist(grp1_all[variable],alpha=0.5, label='group 1',bins=bins)\n",
    "    plt.hist(grp3_4[variable], alpha=0.5, label='group 3 and 4',bins=bins)\n",
    "    plt.hist(grp2_all[variable], alpha=0.5, label='group 2',bins=bins)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('distribution of '+ variable + ' between group 1, group 2 and groups 3 + 4')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can explore the income distribution between the 3 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of income\n",
    "plot_hist('income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the 3 segments, most people fall within the middle range of income (50K - 100K). The income distribution between the 3 segments are relatively similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ditribution of age\n",
    "plot_hist('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age distribution looks relatively similar between the 3 groups as well, with most people between the age 40-80 years old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of amount spent given an effective offer\n",
    "plot_hist('amount',bins=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 2 are people who did not spend at all as the offers were ineffective on them, hence they are not in the graph. But for groups 1 and 3+4, we can see that the amount spent is relatively similar, except that people in group 1 spent slightly more. This is to be expected as we might expect that the offers managed to incentivise them to purchase more, hence their overall spend increased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot tenure of membership\n",
    "plot_hist('membership_tenure_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of membership tenure also looks similar between the 3 segments, with most people between 0-700 days of tenure. It appears as though there are not much demographic characteristic differences between the 3 groups, at least in the current data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Potential all-in-one model\n",
    "\n",
    "Out of curiosity, I wondered if we could predict the effectiveness of an offer if the offer type was included as a categorical feature. Would the type of offer affect the user's responsiveness?\n",
    "\n",
    "To do this, I would need to do some minor data preparation to prepare the data for a multiclass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append datasets together\n",
    "offers_bogo['offer_type']='bogo'\n",
    "offers_info['offer_type']='informational'\n",
    "offers_discount['offer_type']='discount'\n",
    "offers=offers_discount.append(offers_bogo,sort=False)\n",
    "offers=offers.append(offers_info,sort=False)\n",
    "\n",
    "#create dummy variable for offer_type categorical variable\n",
    "offers=dummy(offers,'offer_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do grid search to find optimal parameters for RF model\n",
    "drop_cols_prep=['person','offer_id','effective_offer','amount_invalid']\n",
    "features,target=data_prep(offers,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "rand_forest_param_selection(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols_prep=['person','offer_id','effective_offer','amount_invalid']\n",
    "features,target=data_prep(offers,drop_cols_prep)\n",
    "X_train, X_test, y_train, y_test=model_pipeline(features,target)\n",
    "\n",
    "#Initialize the model\n",
    "all_in_one = RandomForestClassifier(random_state=5,criterion='gini',max_depth= 20, max_features= 'auto',min_samples_split= 2,n_estimators=50,min_samples_leaf=15)\n",
    "\n",
    "results=pd.concat([results[:],run_model(baseline,all_in_one,'all_in_one')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing best performance of all 3 models with all_in_one model\n",
    "results[['RandomForestClassifier_bogo_3','RandomForestClassifier_discount_3','RandomForestClassifier_info_2','DecisionTreeClassifier_all_in_one','RandomForestClassifier_all_in_one']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[['testing_score'],['RandomForestClassifier_bogo_3','RandomForestClassifier_discount_3','RandomForestClassifier_info_2','DecisionTreeClassifier_all_in_one','RandomForestClassifier_all_in_one']].plot.bar()\n",
    "plt.title('Comparing testing set accuracy score for the 3 models vs all-in-one model')\n",
    "plt.legend(loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the performance of the 3 best models for each offer type with the all_in_one model, we can se that having the all-in-one model is not as good as the RF bogo and discount models, and is about slightly better than the info model. This is probably due to the info model pulling down the performance, resulting in lower accuracy for the all in one model. I suspect that if we were to break down the all-in-one model performance to just looking at its ability to predict the effectiveness of informational offer types, it would also be worse than its performance predicting the other 2 types.  \n",
    "\n",
    "If we take a step back and look at the big picture, it is more useful to have a higher accuracy for 3 separate models, as opposed to one all-in-one model. This is because the BOGO and discount offers are actually aimed at driving sales with some promotional cost, whereas the informational offer is essentially 'free' with no cost, and if they can drive sales that would be a bonus.\n",
    "\n",
    "Hence, I would actually suggest that the 3 separate models are more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Given an effective offer, can we predict how much someone would spend? \n",
    "\n",
    "In addition to the all-in-one model, since we already kept the datasets of effective transactions, I was curious to know if I could build a regression model to predict how much someone would spend, given an effective offer. I could have built a model separately for each offer type to predict their spend, but I was curious to know if the type of offer would also determine a user's level of spend. \n",
    "\n",
    "To do this, we have already assigned effective offers based on group 1 customers. From there, we just need to sum up their amount of spend driven by offers to see if we can predict how much someone would spend depending on the offer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append all 3 datasets together\n",
    "grp1=grp1_bogo.append(grp1_discount,sort=False)\n",
    "grp1=grp1.append(grp1_info,sort=False)\n",
    "\n",
    "#drop unnecessary columns\n",
    "drop_cols('effective_offer',grp1,inplace=True)\n",
    "\n",
    "#get offer details\n",
    "grp1=grp1.merge(portfolio,how='left',on='offer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only take into account transactions that are influenced by an offer (i.e. `valid_completed=1`) as we want to predict the spend given (i.e. based on) the influence of an effective offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get sum of valid transactions per person based on unique person and offer_id pair\n",
    "grp1=grp1.merge(transcript[['person','offer_id','amount']][transcript['valid_completed']==1].groupby(['person','offer_id']).sum(),on=['person','offer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get demographic data and membership_tenure details\n",
    "grp1=member(grp1)\n",
    "\n",
    "#reset index for offers_info\n",
    "grp1=drop_cols('index',grp1.reset_index())\n",
    "\n",
    "#reuse offers_info channel_col function to expand channel column into categorical variables\n",
    "channel_col('web',grp1)\n",
    "channel_col('email',grp1)\n",
    "channel_col('mobile',grp1)\n",
    "channel_col('social',grp1);\n",
    "\n",
    "drop_cols('channels',grp1,inplace=True);\n",
    "\n",
    "#reuse offers_info function to prep dataset\n",
    "grp1=prep_offers_df(grp1)\n",
    "\n",
    "#encode offer type as dummy variables\n",
    "grp1=dummy(grp1,'offer_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a regression model, we need to prevent multicollinearity by reducing the level of the dummy variables by 1, dropping those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add one level of dummy variable to drop\n",
    "drop_cols_prep=['person', 'offer_id','amount','social','gender_O','offer_type_informational']\n",
    "target=grp1['amount']\n",
    "features=drop_cols(drop_cols_prep,grp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a regression model, we need to change the metrics such that it is not a classification model. Hence, I tweak my `train_predict` and `run_model` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweak train_predict function -\n",
    "def train_predict_reg(learner, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: review_scores_rating training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: review_scores_rating testing set\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    #Fit the learner to the training data and get training time\n",
    "    start = time() \n",
    "    learner = learner.fit(X_train, y_train)\n",
    "    end = time() \n",
    "    results['train_time'] = end-start\n",
    "    \n",
    "    # Get predictions on the test set(X_test), then get predictions on first 300 training samples\n",
    "    start = time() \n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train)\n",
    "    end = time() \n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end-start\n",
    "    \n",
    "    #add training accuracy to results\n",
    "    results['training_score']=learner.score(X_train,y_train)\n",
    "    \n",
    "    #add testing accuracy to results\n",
    "    results['testing_score']=learner.score(X_test,y_test)\n",
    "    \n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, len(y_train)))\n",
    "    print(\"MSE_train: %.4f\" % mean_squared_error(y_train,predictions_train))\n",
    "    print(\"MSE_test: %.4f\" % mean_squared_error(y_test,predictions_test))\n",
    "    print(\"Training accuracy:%.4f\" % results['training_score'])\n",
    "    print(\"Test accuracy:%.4f\" % results['testing_score'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_reg(clf1,clf2,name):\n",
    "    '''\n",
    "    input:\n",
    "    - clf1: baseline regression model\n",
    "    - clf2: 2nd regression model to compare\n",
    "    - name: name to keep track of comparison\n",
    "    output:\n",
    "    - dataframe containing results of training and prediction of model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Collect results on the learners\n",
    "    results = {}\n",
    "    for clf in [clf1, clf2]:\n",
    "        clf_name = clf.__class__.__name__ + '_' +name\n",
    "        results[clf_name] = {}\n",
    "        results[clf_name]= train_predict_reg(clf, X_train, y_train, X_test, y_test)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=model_pipeline_poly(features,target,2)\n",
    "\n",
    "#Initialize the model\n",
    "clf1 = Ridge(alpha=2,random_state=2)\n",
    "clf2 = DecisionTreeRegressor(random_state=2)\n",
    "\n",
    "results_reg=run_model_reg(clf1,clf2,'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression models really underperformed in terms of predicting the amount spent. It appears with the current data within our group 1 of customers, there is not enough information to predict the amount that can be driven by the offer type. We can see the Decision Tree Regressor model really overfit the data, with a very high training score but sub par testing score. Meanwhile, the linear regression model (with ridge/l2 regularization) also shows a minimal correlation between the features and the target variable. The model really underfits the data.\n",
    "\n",
    "I may get better performance if I break the models up into 3 different models based on offer type again; or even try to include non-influenced/invalid transactions, but this could be an exploration for another time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall, I found this project challenging, mainly due to the structure of the data in the `transcript` dataset. I had started out with 2 business questions:\n",
    "\n",
    "1. What are the main drivers of an effective offer on the Starbucks app?\n",
    "2. Could the data provided, namely offer characteristics and user demographics, predict whether a user would take up an offer?\n",
    "\n",
    "### a. Reflection:\n",
    "\n",
    "#### a.i. Question 1 findings:\n",
    "Regarding Question 1, the feature importance given by all three models was that the tenure of a member is the most significant predictor of the effectiveness of an offer. Further study could determine the average tenure days that would result in an effective BOGO offer.\n",
    "\n",
    "For all three models, the top three variables were the same - membership tenure, income, and age. However, the order of importance for income and age switched depending on the offer type.\n",
    "\n",
    "For BOGO and discount offers, the distribution of feature importances was relatively equal. However, for informational offers, the distribution was slightly more balanced, with income being the second most important variable.\n",
    "\n",
    "#### a.ii. Question 2 findings:\n",
    "\n",
    "Using three separate models to predict the effectiveness of each offer type resulted in good accuracy for the BOGO and discount models (82.83% for BOGO and 87.35% for discount), while slightly less accurate performance for informational offers (75.3%).\n",
    "\n",
    "In a business setting, I would consider 75% accuracy acceptable for informational offers, as there is no cost involved in informing users about a product.\n",
    "\n",
    "For BOGO and discount models, I am satisfied with the 80% and above accuracy, as in a business setting, it would be acceptable to show offers to people, even if the model misclassifies a few cases. The overall revenue increase might justify the few mistakes.\n",
    "\n",
    "### b. Main challenges and potential improvement:\n",
    "\n",
    "When analysing and building the machine learning models to answer the above questions, reflections on my main challenges and findings are as follows:\n",
    "\n",
    "#### b.i. Attribution framework for assigning offer_ids for transactions:\n",
    "\n",
    "In order to answer Question 1, I had to first define what an 'effective offer' means using the transactional records. This proved to be the trickiest portion of the project. I had to define a funnel for what what an effective conversion would look like, as we had data on both effective and noneffective conversions. Thus, I was desigining an attribution model for the conversion events (`offer completed` and `transaction` events) based on the events that occurred prior for each person.\n",
    "\n",
    "I ended up having to separate the users into 4 different pools, based on their actions in the transcript data:\n",
    "\n",
    "- Group 1: People who are influenced by offers and thus purchase/complete the offer(successful/effective conversion of offer)\n",
    "- Group 2: People who receive and an offer but is not influenced and thus no conversion event (ineffective conversion of offer)\n",
    "- Group 3: People who have conversion events but was not actually influenced by an offer\n",
    "- Group 4: People who receive offers but no views or action taken\n",
    "\n",
    "Even after separating the groups, it was challenging to assign the people in group 3 based on the transactional data. I had to define the event space where the right sequence of events would occur before I could assign an offer id to transactions (which did not have an offer_id), essentally designing a event/sequence-based attribution window.\n",
    "\n",
    "After attributing the conversions to specific offers, the rest of the data preparation and cleaning was relatively straightforward. I was grateful that there were not many missing values, and the preparation of categorical variables was also relatively straightforward.\n",
    "\n",
    "#### b.ii. Feature engineering:\n",
    "\n",
    "After finding that the model had slightly underfit on my first attempt in this project, I decided to do some basic feature engineering. I added this section later, and it improved the model's performance slightly. One feature I engineered was membership_tenure, which I created from the became_member_on column. This feature turned out to be the most important predictor variable.\n",
    "\n",
    "However, I found that I could not think of additional features using the time data, even though I suspected that the time of receiving the offer might be influential in determining its effectiveness.\n",
    "\n",
    "#### b.iii. Model implementation decisions:\n",
    "\n",
    "I decided to build three separate models based on offer types, as I wanted to discover what drives an effective offer. Separating the data into offer types helped to remove noise from the data. This decision turned out to be a good one, as the single BOGO and discount models had good performance in testing scores compared to the all-in-one model's overall score.\n",
    "\n",
    "For the info model, the accuracy was slightly worse, as there were fewer records overall (half of the BOGO and discount models). With more data, I believe that the accuracy could have been higher. As I made decisions to improve the model fit, such as adding polynomial features and removing noisy features like the amount_invalid feature, a clear diverging pattern occurred between the training and testing scores. Due to the limited data, the model ended up overfitting, and I believe that more data would have improved the model accuracy.\n",
    "\n",
    "Regarding model selection, I chose tree-based models to assess feature importance. However, I could have extended this study further by testing a parametric/regression model, such as logistic regression for classification tasks. The weights of the coefficients from a regression model might have been interesting to contrast with the feature importance of a tree-based model, given that both models analyze the data differently. The feature membership_tenure_days might not have been the most highly weighted feature in a regression model, in contrast to how it was in this study.\n",
    "\n",
    "#### b.iv. Exploring demographics of different customer groups:\n",
    "\n",
    "Out of curiosity, I examined the characteristics of groups 3 and 4, which are customers who are not influenced by offers. However, upon comparing their demographics with groups 1 and 2, I did not find any significant differences.\n",
    "\n",
    "I would have liked to have more data to understand why this group of customers tends to not be influenced by offers. This would have enabled me to make useful suggestions on how to provide a good customer experience to these customers, even if we do not serve them any offers.\n",
    "\n",
    "#### b.v. Model accuracy in predicting amount spent given an effective offer:\n",
    "\n",
    "I attempted to build a regression model to predict how much a user would spend, given that they were effectively influenced by an offer. The motivation behind this was to assess which offers bring in the most revenue. I used offer characteristics and demographics of app users as features to build the model.\n",
    "\n",
    "However, the results of the model showed almost no correlation between the provided features and the amount spent per user. The features used were not strong enough predictors for the amount spent per user. It is possible that including the value of the offer, such as the dollar value of a discount offer, might improve the predictive power of the model.\n",
    "\n",
    "To improve the model's accuracy, I could have broken the data into three different models for the three offer types, as I did with the binary classification models. However, as this was just a curiosity and I was exploring whether the offer type would be a statistically significant predictor feature, I built an all-in-one model for this instance.\n",
    "\n",
    "Given more time and data, it would be worth exploring this further to determine if a more accurate model can be built."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
